{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437243dd-3def-48b6-8abd-5011256dc4c9",
   "metadata": {},
   "source": [
    "## 使用学习率调度器的目的\n",
    "在训练的不同阶段，使用不同的学习率来训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969f216-3c54-41fd-8c49-c563844d2f7e",
   "metadata": {},
   "source": [
    "# 代码举例1：optim.lr_scheduler.StepLR\n",
    "每间隔 step_size 个epoch，调度器会将在上一个学习率的基础上，乘以一个指定的调整因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c284df-ebde-4d10-99e4-f17457bc51ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Learning Rate: 0.01\n",
      "Epoch 2, Learning Rate: 0.01\n",
      "Epoch 3, Learning Rate: 0.01\n",
      "Epoch 4, Learning Rate: 0.01\n",
      "Epoch 5, Learning Rate: 0.005\n",
      "Epoch 6, Learning Rate: 0.005\n",
      "Epoch 7, Learning Rate: 0.005\n",
      "Epoch 8, Learning Rate: 0.005\n",
      "Epoch 9, Learning Rate: 0.005\n",
      "Epoch 10, Learning Rate: 0.0025\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 每5个step，学习率衰减为上次的一半\n",
    "decay_factor = 0.5\n",
    "decay_steps = 5\n",
    "lr_scheduler = StepLR(optimizer, step_size=decay_steps, gamma=decay_factor)\n",
    "\n",
    "# 在每个epoch结束时调用\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练...\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch + 1}, Learning Rate: {current_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775df7a7-fc43-4133-b0fc-6b3d41081453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fe31547-3f47-4526-988b-1e74164ff580",
   "metadata": {},
   "source": [
    "# 代码举例2：optim.lr_scheduler.LambdaLR\n",
    "允许用户自定义 学习率调整函数 ，通过学习率调整函数 返回的调整因子$$\\gamma$$ 来调整学习率 ： 在初始化学习率$$initial\\_lr$$ 的基础上，乘以 调整因子$$\\gamma$$，得到调整后的 学习率\n",
    "更新策略 ：  $$new\\_lr = initial\\_lr \\times \\lambda$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8f2d866-a47c-49a9-8c76-6613c0980fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Learning Rate: 0.095\n",
      "Epoch 2, Learning Rate: 0.09025\n",
      "Epoch 3, Learning Rate: 0.0857375\n",
      "Epoch 4, Learning Rate: 0.081450625\n",
      "Epoch 5, Learning Rate: 0.07737809374999999\n",
      "Epoch 6, Learning Rate: 0.07350918906249998\n",
      "Epoch 7, Learning Rate: 0.06983372960937498\n",
      "Epoch 8, Learning Rate: 0.06634204312890622\n",
      "Epoch 9, Learning Rate: 0.0630249409724609\n",
      "Epoch 10, Learning Rate: 0.05987369392383787\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 定义自定义的学习率调整函数\n",
    "lr_lambda = lambda epoch: 0.95 ** epoch  # 以0.95的指数衰减\n",
    "\n",
    "# 定义学习率调度器\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# 模拟训练过程\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练...\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch + 1}, Learning Rate: {current_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470aa7a7-b7ef-4af6-8e27-ef664a44a42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ed73bcf-6ff2-421f-a192-e1bd7e5f9a1b",
   "metadata": {},
   "source": [
    "# 代码举例3：optim.lr_scheduler.ExponentialLR\n",
    "每个 epoch 都做一次更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7adfae3a-fb59-482d-a5b6-fa13761dd3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Learning Rate: 0.005\n",
      "Epoch 2, Learning Rate: 0.0025\n",
      "Epoch 3, Learning Rate: 0.00125\n",
      "Epoch 4, Learning Rate: 0.000625\n",
      "Epoch 5, Learning Rate: 0.0003125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 学习率调度器\n",
    "decay_factor = 0.5\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=decay_factor)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练...\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch + 1}, Learning Rate: {current_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df4ac0-86aa-4128-8f76-c657c3271869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0e1ec-5658-4def-8f5a-ac73897cb707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "079c6465-700d-4e8b-8221-2372feae7658",
   "metadata": {},
   "source": [
    "# 代码模板1：optim.lr_scheduler.StepLR\n",
    "每间隔 step_size 个epoch，调度器会将在上一个学习率的基础上，乘以一个指定的调整因子$$\\gamma$$\n",
    "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)\n",
    "- optimizer: 需要进行学习率调度的优化器\n",
    "- step_size: 每隔多少个 steps (epoch) 调整一次学习率。steps 从 0 开始，每调用一次 lr_scheduler.step()，steps 自增1，当 steps//step_size=0 时，学习率调整一次\n",
    "- gamma: 学习率衰减因子，即学习率每次调整时乘以的值，默认为 0.1\n",
    "- last_epoch:  上一个 epoch 的索引，比如，当训练了几个epoch之后中断了，想要继续训练，就会在 last_epoch 的基础上，继续训练。默认为 -1，表示从头开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d4b537-e250-4411-934b-1f0bdd21e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# 定义模型\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "# 实例化模型\n",
    "model = MyModel()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# ***************************************************************************关键模块*************************************************************\n",
    "# 每5个step，学习率衰减为上次的一半\n",
    "decay_factor = 0.5\n",
    "decay_steps = 5\n",
    "lr_scheduler = StepLR(optimizer, step_size=decay_steps, gamma=decay_factor)\n",
    "\n",
    "# 在每个epoch结束时调用\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练...\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    # 获取当前的学习率\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542cadf-f826-4d51-9833-c73495160418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a4cc676-14cb-48e4-8cad-5a50640428da",
   "metadata": {},
   "source": [
    "# 代码模板2：optim.lr_scheduler.LambdaLR\n",
    "允许用户自定义 学习率调整函数 ，通过学习率调整函数 返回的调整因子$$\\gamma$$ 来调整学习率 ： 在初始化学习率$$initial\\_lr$$ 的基础上，乘以 调整因子$$\\gamma$$，得到调整后的 学习率\n",
    "更新策略 ：  $$new\\_lr = initial\\_lr \\times \\lambda$$\n",
    "参数：\n",
    "- optimizer: 需要进行学习率调度的优化器\n",
    "- lr_lambda: 一个 lambda 函数，输入为 epoch ，返回相应的学习率调整因子 $$\\lambda$$\n",
    "- last_epoch: 上一个 epoch 的索引，比如，当训练了几个epoch之后中断了，想要继续训练，就会在 last_epoch 的基础上，继续训练。默认为 -1，表示从头开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57082531-5da9-454a-833a-763b66a29430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Learning Rate: 0.095\n",
      "Epoch 2, Learning Rate: 0.09025\n",
      "Epoch 3, Learning Rate: 0.0857375\n",
      "Epoch 4, Learning Rate: 0.081450625\n",
      "Epoch 5, Learning Rate: 0.07737809374999999\n",
      "Epoch 6, Learning Rate: 0.07350918906249998\n",
      "Epoch 7, Learning Rate: 0.06983372960937498\n",
      "Epoch 8, Learning Rate: 0.06634204312890622\n",
      "Epoch 9, Learning Rate: 0.0630249409724609\n",
      "Epoch 10, Learning Rate: 0.05987369392383787\n"
     ]
    }
   ],
   "source": [
    "# 使用举例 （1）：不设置 last_epoch 的情况\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 定义自定义的学习率调整函数\n",
    "lr_lambda = lambda epoch: 0.95 ** epoch  # 以0.95的指数衰减\n",
    "\n",
    "# 定义学习率调度器\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# 模拟训练过程\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练...\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch + 1}, Learning Rate: {current_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038ac35-c726-46b6-a1ed-afa6faefab20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5fb5c6d-f12f-48d0-9e50-b94a1465bf1c",
   "metadata": {},
   "source": [
    "# 代码模板3：optim.lr_scheduler.ExponentialLR\n",
    "每个 epoch 都做一次更新\n",
    "更新策略：$$new\\_lr = initial\\_lr \\times \\gamma^{epoch}$$\n",
    "参数：\n",
    "- optimizer: 需要进行学习率调度的优化器\n",
    "- gamma: 学习率衰减因子，即学习率每次调整时乘以的值，默认为 0.1\n",
    "- last_epoch: 上一个 epoch 的索引，比如，当训练了几个epoch之后中断了，想要继续训练，就会在 last_epoch 的基础上，继续训练。默认为 -1，表示从头开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8d7a6a6-1936-4a21-a43d-6c8628b991a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Learning Rate: 0.005\n",
      "Epoch 2, Learning Rate: 0.0025\n",
      "Epoch 3, Learning Rate: 0.00125\n",
      "Epoch 4, Learning Rate: 0.000625\n",
      "Epoch 5, Learning Rate: 0.0003125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 学习率调度器\n",
    "decay_factor = 0.5\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=decay_factor)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练...\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch + 1}, Learning Rate: {current_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecd7a0-2ab4-4072-9539-d30a48718bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch22",
   "language": "python",
   "name": "pytorch22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
